"""
Enhanced Voice Recognition for Butler Assistant
Optimized for Raspberry Pi with better sensitivity
"""

import speech_recognition as sr
import numpy as np
import sounddevice as sd
import scipy.signal as signal
import queue
import threading
import time
import logging
from collections import deque
from dataclasses import dataclass
from typing import Optional, Tuple, List

logger = logging.getLogger(__name__)

@dataclass
class AudioConfig:
    """Audio configuration for Raspberry Pi optimization"""
    sample_rate: int = 16000  # Lower for RPi optimization
    chunk_size: int = 1024
    channels: int = 1
    dtype: str = 'int16'
    
    # Noise reduction
    noise_reduction_db: float = 20.0
    voice_threshold_db: float = -30.0
    
    # Speech detection
    silence_limit: float = 1.0  # seconds of silence to stop
    energy_threshold: int = 300  # Start with this
    dynamic_energy_threshold: bool = True
    dynamic_energy_adjustment_damping: float = 0.15
    pause_threshold: float = 0.8  # seconds of non-speaking before phrase complete
    
    # Language models
    languages: List[str] = None
    
    def __post_init__(self):
        if self.languages is None:
            self.languages = ['en-IN', 'en-US', 'hi-IN', 'mr-IN']

class EnhancedVoiceRecognizer:
    """
    Advanced voice recognizer with noise reduction and better sensitivity
    """
    
    def __init__(self, config: Optional[AudioConfig] = None):
        self.config = config or AudioConfig()
        self.recognizer = sr.Recognizer()
        
        # Configure recognizer
        self.recognizer.energy_threshold = self.config.energy_threshold
        self.recognizer.dynamic_energy_threshold = self.config.dynamic_energy_threshold
        self.recognizer.dynamic_energy_adjustment_damping = self.config.dynamic_energy_adjustment_damping
        self.recognizer.pause_threshold = self.config.pause_threshold
        
        # Audio queues for real-time processing
        self.audio_queue = queue.Queue(maxsize=20)
        self.processing_queue = queue.Queue()
        
        # State management
        self.is_listening = False
        self.noise_profile = None
        self.audio_history = deque(maxlen=100)
        
        # Threads
        self.capture_thread = None
        self.process_thread = None
        
        # Performance metrics
        self.speech_detected = False
        self.speech_start_time = 0
        self.silence_duration = 0
        
        logger.info("‚úÖ Enhanced Voice Recognizer Initialized")
    
    def apply_noise_reduction(self, audio_data: np.ndarray) -> np.ndarray:
        """
        Apply real-time noise reduction using spectral gating
        """
        try:
            # Convert to frequency domain
            audio_float = audio_data.astype(np.float32) / 32768.0
            
            # Simple spectral noise reduction
            if len(audio_float) > 0:
                # Apply a bandpass filter for human voice (300-3400 Hz)
                nyquist = self.config.sample_rate / 2
                low = 300 / nyquist
                high = 3400 / nyquist
                b, a = signal.butter(4, [low, high], btype='band')
                filtered = signal.filtfilt(b, a, audio_float)
                
                # Normalize
                if np.max(np.abs(filtered)) > 0:
                    filtered = filtered / np.max(np.abs(filtered))
                
                return (filtered * 32767).astype(np.int16)
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Noise reduction failed: {e}")
        
        return audio_data
    
    def detect_voice_activity(self, audio_data: np.ndarray) -> bool:
        """
        Improved Voice Activity Detection (VAD)
        """
        try:
            # Convert to float
            audio_float = audio_data.astype(np.float32) / 32768.0
            
            # Calculate energy
            energy = np.mean(audio_float ** 2)
            
            # Convert to dB
            energy_db = 10 * np.log10(energy + 1e-10)
            
            # Store in history for adaptive threshold
            self.audio_history.append(energy_db)
            
            # Adaptive threshold based on recent history
            if len(self.audio_history) > 10:
                background_noise = np.percentile(list(self.audio_history)[-10:], 30)
                threshold = background_noise + 10  # 10 dB above background
                
                # Check if current energy exceeds threshold
                if energy_db > threshold:
                    if not self.speech_detected:
                        self.speech_detected = True
                        self.speech_start_time = time.time()
                        self.silence_duration = 0
                    return True
                else:
                    if self.speech_detected:
                        self.silence_duration += len(audio_data) / self.config.sample_rate
                        
                        # If silence continues for too long, stop speech
                        if self.silence_duration > self.config.silence_limit:
                            self.speech_detected = False
                    return False
            
        except Exception as e:
            logger.debug(f"VAD calculation error: {e}")
        
        return False
    
    def capture_audio_stream(self):
        """
        Capture audio in real-time from microphone
        """
        logger.info("üé§ Starting audio capture stream...")
        
        def audio_callback(indata, frames, time_info, status):
            if status:
                logger.warning(f"Audio status: {status}")
            
            if self.is_listening:
                # Convert to numpy array
                audio_array = indata.copy()
                
                # Apply noise reduction
                cleaned_audio = self.apply_noise_reduction(audio_array.flatten())
                
                # Detect voice activity
                has_voice = self.detect_voice_activity(cleaned_audio)
                
                if has_voice:
                    # Put in queue for processing
                    try:
                        self.audio_queue.put(cleaned_audio, timeout=0.1)
                    except queue.Full:
                        pass  # Skip if queue is full
        
        try:
            with sd.InputStream(
                samplerate=self.config.sample_rate,
                channels=self.config.channels,
                dtype=self.config.dtype,
                callback=audio_callback,
                blocksize=self.config.chunk_size
            ):
                while self.is_listening:
                    time.sleep(0.1)
                    
        except Exception as e:
            logger.error(f"‚ùå Audio stream error: {e}")
    
    def process_audio_chunks(self):
        """
        Process accumulated audio chunks into speech
        """
        logger.info("üîÑ Starting audio processing thread...")
        
        accumulated_audio = []
        last_voice_time = time.time()
        
        while self.is_listening:
            try:
                # Get audio chunk with timeout
                audio_chunk = self.audio_queue.get(timeout=0.5)
                
                # Add to accumulated audio
                accumulated_audio.append(audio_chunk)
                last_voice_time = time.time()
                
                # If we have enough audio, process it
                if len(accumulated_audio) > 5:  # ~0.3 seconds
                    # Convert to single array
                    full_audio = np.concatenate(accumulated_audio)
                    
                    # Convert to AudioData for speech_recognition
                    audio_data = sr.AudioData(
                        full_audio.tobytes(),
                        self.config.sample_rate,
                        2  # Sample width in bytes
                    )
                    
                    # Put for recognition
                    self.processing_queue.put(audio_data)
                    
                    # Reset accumulation
                    accumulated_audio = []
                
                # Check for end of speech
                if time.time() - last_voice_time > 0.5:  # 500ms silence
                    if accumulated_audio:
                        # Process remaining audio
                        full_audio = np.concatenate(accumulated_audio)
                        audio_data = sr.AudioData(
                            full_audio.tobytes(),
                            self.config.sample_rate,
                            2
                        )
                        self.processing_queue.put(audio_data)
                        accumulated_audio = []
                    
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"‚ùå Audio processing error: {e}")
    
    def recognize_speech_realtime(self, timeout: float = 10.0) -> Tuple[bool, str, str]:
        """
        Real-time speech recognition with multiple language attempts
        Returns: (success, text, language)
        """
        logger.info("üé§ Listening in real-time mode...")
        
        self.is_listening = True
        
        # Start capture thread
        self.capture_thread = threading.Thread(target=self.capture_audio_stream)
        self.capture_thread.daemon = True
        self.capture_thread.start()
        
        # Start processing thread
        self.process_thread = threading.Thread(target=self.process_audio_chunks)
        self.process_thread.daemon = True
        self.process_thread.start()
        
        start_time = time.time()
        best_result = None
        best_confidence = 0
        detected_lang = 'en-IN'
        
        try:
            while time.time() - start_time < timeout:
                try:
                    # Get audio data from processing queue
                    audio_data = self.processing_queue.get(timeout=1.0)
                    
                    # Try recognition in multiple languages
                    for language in self.config.languages:
                        try:
                            text = self.recognizer.recognize_google(
                                audio_data,
                                language=language,
                                show_all=False
                            )
                            
                            if text:
                                # Simple confidence scoring (length heuristic)
                                confidence = len(text.split()) / 10.0
                                
                                if confidence > best_confidence:
                                    best_result = text
                                    best_confidence = confidence
                                    detected_lang = language
                                    
                                    logger.info(f"‚úÖ Recognized ({language}): '{text[:50]}...'")
                                    
                                    # If we get a good result, return early
                                    if confidence > 0.3:
                                        self.is_listening = False
                                        return True, text, language
                                
                        except sr.UnknownValueError:
                            continue
                        except sr.RequestError as e:
                            logger.warning(f"‚ö†Ô∏è API error for {language}: {e}")
                            continue
                    
                except queue.Empty:
                    # No audio to process, continue listening
                    continue
                    
        except Exception as e:
            logger.error(f"‚ùå Recognition error: {e}")
        finally:
            self.is_listening = False
            
            # Wait for threads to finish
            if self.capture_thread:
                self.capture_thread.join(timeout=2)
            if self.process_thread:
                self.process_thread.join(timeout=2)
        
        if best_result:
            return True, best_result, detected_lang
        else:
            return False, "", "en-IN"
    
    def calibrate_microphone(self, duration: float = 3.0):
        """
        Calibrate microphone for ambient noise
        """
        logger.info("üîß Calibrating microphone...")
        
        print("\n" + "="*50)
        print("üé§ MICROPHONE CALIBRATION")
        print("="*50)
        print("Please remain silent for 3 seconds...")
        
        # Record ambient noise
        with sr.Microphone() as source:
            self.recognizer.adjust_for_ambient_noise(source, duration=duration)
        
        # Get the adjusted energy threshold
        adjusted_threshold = self.recognizer.energy_threshold
        logger.info(f"üìä Adjusted energy threshold: {adjusted_threshold:.2f}")
        
        print(f"‚úÖ Calibration complete!")
        print(f"üìä Sensitivity: {adjusted_threshold:.2f}")
        print("="*50)
        
        return adjusted_threshold
    
    def quick_listen(self, timeout: float = 5.0) -> Tuple[bool, str]:
        """
        Quick listen mode - simple one-shot recognition
        """
        try:
            with sr.Microphone() as source:
                print(f"\nüé§ Listening for {timeout} seconds...")
                
                # Adjust for noise
                self.recognizer.adjust_for_ambient_noise(source, duration=0.5)
                
                # Listen
                audio = self.recognizer.listen(source, timeout=timeout)
                
                # Try English first
                try:
                    text = self.recognizer.recognize_google(audio, language='en-IN')
                    return True, text
                except sr.UnknownValueError:
                    # Try Hindi
                    try:
                        text = self.recognizer.recognize_google(audio, language='hi-IN')
                        return True, text
                    except:
                        return False, ""
                        
        except sr.WaitTimeoutError:
            return False, ""
        except Exception as e:
            logger.error(f"‚ùå Quick listen error: {e}")
            return False, ""


# Singleton instance for easy access
enhanced_recognizer = EnhancedVoiceRecognizer()

if __name__ == "__main__":
    # Test the enhanced recognizer
    import sys
    
    logging.basicConfig(level=logging.INFO)
    
    print("\n" + "="*60)
    print("üîä ENHANCED VOICE RECOGNITION TEST")
    print("="*60)
    
    evr = EnhancedVoiceRecognizer()
    
    # Calibrate
    evr.calibrate_microphone()
    
    print("\nChoose mode:")
    print("1. Quick Listen (5 seconds)")
    print("2. Real-time Listening (continuous)")
    print("3. Sensitivity Test")
    
    choice = input("\nEnter choice (1-3): ").strip()
    
    if choice == "1":
        print("\nüé§ Speak now...")
        success, text = evr.quick_listen()
        
        if success:
            print(f"\n‚úÖ Recognized: '{text}'")
        else:
            print("\n‚ùå No speech detected")
    
    elif choice == "2":
        print("\nüé§ Real-time listening (speak naturally)...")
        print("Press Ctrl+C to stop\n")
        
        success, text, lang = evr.recognize_speech_realtime(timeout=15)
        
        if success:
            print(f"\n‚úÖ Recognized ({lang}): '{text}'")
        else:
            print("\n‚ùå No speech detected")
    
    elif choice == "3":
        print("\nüé§ Sensitivity test - speak at different volumes")
        print("Testing for 10 seconds...")
        
        evr.calibrate_microphone()
        
        # Test with simple listen
        success, text = evr.quick_listen(timeout=10)
        
        if success:
            print(f"\n‚úÖ Sensitivity OK: '{text}'")
        else:
            print("\n‚ö†Ô∏è Low sensitivity - try increasing mic volume")
    
    print("\n" + "="*60)
